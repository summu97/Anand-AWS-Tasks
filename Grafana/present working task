Install postgresql using docker:

docker run -itd --name postgres -e POSTGRES_USER=admin -e POSTGRES_PASSWORD=admin123 -e POSTGRES_DB=monitoringdb -p 5432:5432 postgres:14
================================================================================
Install Grafana Using Helm:
---
Command to install helm: 
curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
---
Add Grafana Helm repo:
helm repo add grafana https://grafana.github.io/helm-charts
helm repo update
---
Install Grafana:
helm install grafana grafana/grafana --namespace monitoring --create-namespace --set adminUser=admin --set adminPassword=admin123 --set service.type=NodePort

This will:
Create the monitoring namespace (if not exists)
Set Grafana admin user to admin / admin123
Expose the Grafana UI via a NodePort
---
Get Grafana access URL:
If you're using Minikube:
minikube service grafana -n monitoring --url

If you cant access grafana then follow below steps.
---
Change Grafana service to LoadBalancer:
helm upgrade grafana grafana/grafana \
  -n monitoring \
  --set service.type=LoadBalancer
---
minikube tunnel
---
kubectl get svc grafana -n monitoring
---
Use kubectl port-forward:
kubectl port-forward --address 0.0.0.0 svc/grafana -n monitoring 3000:80

http://vm-ip:3000

---
Access Grafana:
Open the URL in your browser.
Login with:
Username: admin
Password: admin123
---
================================================================================
Helm commands to install Prometheus
---
Add Prometheus Helm Repo:
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update
---
Create Namespace (optional but recommended):
kubectl create namespace monitoring
---
Install Prometheus Using Helm:

Helm command to expose Prometheus using a LoadBalancer service:
helm install prometheus prometheus-community/prometheus \
  -n monitoring \
  --set server.service.type=LoadBalancer

minikube tunnel

This will install:
Prometheus server
Alertmanager
Node Exporter
Pushgateway
---
Verify Installation:
kubectl get pods -n monitoring
kubectl get svc -n monitoring
---
Access Prometheus UI:
Port-forward Prometheus server:
kubectl port-forward --address 0.0.0.0 svc/prometheus-server -n monitoring 9090:80


minikube tunnel

---


================================================================================
Objective:
You want to scrape the following PostgreSQL query performance metrics into Prometheus â†’ Grafana:
pg_stat_statements_query_stats_calls
pg_stat_statements_query_stats_max_time
pg_stat_statements_query_stats_mean_time
pg_stat_statements_query_duration_histogram_bucket
pg_stat_statements_query_duration_histogram_count
pg_stat_statements_query_duration_histogram_sum
p95 / p99 percentiles for query duration
-----------------
Solution:
We'll use the postgres_exporter from prometheus-community and a custom queries.yaml file that:
Scrapes pg_stat_statements
Computes histogram data needed for p95/p99
================================================================================
Steps to scrape PostgreSQL metrics:
Enable pg_stat_statements extension in PostgreSQL
docker exec -it <postgres_container_name> psql -U <your_postgres_user> -d <your_db>
EX:
docker exec -it postgres psql -U admin -d monitoringdb
Above command will connect you to database and now directly run below command.

Then run:
CREATE EXTENSION IF NOT EXISTS pg_stat_statements;

ADD these to config file:
And ensure these lines are in postgresql.conf:
shared_preload_libraries = 'pg_stat_statements'
track_activity_query_size = 2048

Steps:
Enter the container: docker exec -it postgres bash
Find the postgresql.conf file:
Run: find / -name postgresql.conf
Common locations:
/var/lib/postgresql/data/postgresql.conf
/var/lib/postgresql/data/pgdata/postgresql.conf
/usr/share/postgresql/postgresql.conf.sample (not the active one)

Edit the file using vi or nano (if available):
vi /var/lib/postgresql/data/postgresql.conf

If its not available then:
Update/add the config lines using sed(insie container):
First, check if the config already exists: grep shared_preload_libraries /var/lib/postgresql/data/postgresql.conf
Output:
#shared_preload_libraries = ''  # (change requires restart)


Now run this inside the container to uncomment and set it:
sed -i "s/^#shared_preload_libraries = ''/shared_preload_libraries = 'pg_stat_statements'/" /var/lib/postgresql/data/postgresql.conf

Then check if it was updated correctly: grep shared_preload_libraries /var/lib/postgresql/data/postgresql.conf
It should now show: shared_preload_libraries = 'pg_stat_statements'
---
Now configure track_activity_query_size
Check if it's present: grep track_activity_query_size /var/lib/postgresql/data/postgresql.conf
Output:
#track_activity_query_size = 1024       # (change requires restart)

To update it to 2048, run this inside the container:
sed -i "s/^#track_activity_query_size = 1024/track_activity_query_size = 2048/" /var/lib/postgresql/data/postgresql.conf
Then confirm it worked: grep track_activity_query_size /var/lib/postgresql/data/postgresql.conf
You should now see: track_activity_query_size = 2048

Finally, restart the container to apply the changes:
exit
Then on your host: docker restart postgres

Now pg_stat_statements will fully function with these configs active. You can validate with:
docker exec -it postgres psql -U admin -d monitoringdb -c "SELECT * FROM pg_stat_statements LIMIT 5;"


Output:
 userid | dbid | toplevel | queryid | query | plans | total_plan_time | min_plan_time | max_plan_time | mean_plan_time | stddev_plan_time | calls | total_exec_time | min_exec_time | max_exec_time | mean_exec_time | stddev_exec_time | rows | shared_blks_hit | shared_blks_read | shared_blks_dirtied | shared_blks_written | local_blks_hit | local_blks_read | local_blks_dirtied | local_blks_written | temp_blks_read | temp_blks_written | blk_read_time | blk_write_time | wal_records | wal_fpi | wal_bytes      
--------+------+----------+---------+-------+-------+-----------------+---------------+---------------+----------------+------------------+-------+-----------------+---------------+---------------+----------------+------------------+------+-----------------+------------------+---------------------+---------------------+----------------+-----------------+--------------------+--------------------+----------------+-------------------+---------------+----------------+-------------+---------+----------- 

==================
That output is the column headers from the pg_stat_statements view â€” which means pg_stat_statements is now fully enabled and working
To view actual query performance, just run:
docker exec -it postgres psql -U admin -d monitoringdb -c "SELECT query, calls, mean_exec_time, total_exec_time FROM pg_stat_statements ORDER BY total_exec_time DESC LIMIT 10;"

Output:
                   query                   | calls |   mean_exec_time    |   total_exec_time   
-------------------------------------------+-------+---------------------+---------------------
 SELECT * FROM pg_stat_statements LIMIT $1 |     1 | 0.22200799999999998 | 0.22200799999999998
(1 row)
==================
NOW ITS TIME TO SET EXPORTER:

2ï¸âƒ£ Use Custom queries.yaml for Exporter
Create a file named queries.yaml with this content:
vim queries.yaml

pg_stat_statements:
  query: |
    SELECT
      queryid,
      query,
      calls,
      total_time,
      mean_time,
      max_time
    FROM pg_stat_statements
    WHERE query NOT ILIKE 'EXPLAIN%' AND query NOT ILIKE 'FETCH%'
    ORDER BY total_time DESC
    LIMIT 20;
  metrics:
    - queryid:
        usage: "LABEL"
        description: "Query ID"
    - query:
        usage: "LABEL"
        description: "Query Text"
    - calls:
        usage: "COUNTER"
        description: "Number of times executed"
    - total_time:
        usage: "GAUGE"
        description: "Total time spent"
    - mean_time:
        usage: "GAUGE"
        description: "Average execution time"
    - max_time:
        usage: "GAUGE"
        description: "Maximum execution time"

pg_stat_statements_histogram:
  query: |
    SELECT
      queryid,
      width_bucket(total_time, 0, 1000, 100) AS bucket,
      count(*) AS count,
      sum(total_time) AS sum
    FROM pg_stat_statements
    GROUP BY queryid, bucket;
  metrics:
    - queryid:
        usage: "LABEL"
        description: "Query ID"
    - bucket:
        usage: "LABEL"
        description: "Bucket index (0â€“1000ms)"
    - count:
        usage: "COUNTER"
        description: "Number of queries in this bucket"
    - sum:
        usage: "COUNTER"
        description: "Sum of durations in this bucket"


This gives you bucketed histogram-like metrics for approximating p95/p99.
-----------------
3ï¸âƒ£ Create a ConfigMap in Kubernetes
kubectl create configmap postgres-exporter-queries \
  --from-file=queries.yaml \
  -n monitoring
-----------------
4ï¸âƒ£ Modify Your postgres-exporter.yaml Deployment
Update it like this:
Deploy PostgreSQL Exporter (postgres-exporter.yaml)
vim postgres-exporter.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: postgres-exporter
  namespace: monitoring
spec:
  replicas: 1
  selector:
    matchLabels:
      app: postgres-exporter
  template:
    metadata:
      labels:
        app: postgres-exporter
    spec:
      containers:
      - name: postgres-exporter
        image: quay.io/prometheuscommunity/postgres-exporter
        ports:
        - containerPort: 9187
        env:
        - name: DATA_SOURCE_NAME
          value: "postgresql://admin:admin123@postgres:5432/monitoringdb?sslmode=disable"
        - name: PG_EXPORTER_EXTEND_QUERY_PATH
          value: /etc/postgres-exporter/queries.yaml
        volumeMounts:
        - name: custom-queries
          mountPath: /etc/postgres-exporter/queries.yaml
          subPath: queries.yaml
      volumes:
      - name: custom-queries
        configMap:
          name: postgres-exporter-queries
---
apiVersion: v1
kind: Service
metadata:
  name: postgres-exporter
  namespace: monitoring
spec:
  type: NodePort  # Or LoadBalancer if you're using a cloud environment
  ports:
    - port: 9187
      targetPort: 9187
  selector:
    app: postgres-exporter
------
kubectl apply -f postgres-exporter.yaml

------------------------------------------------------
ADDING postgres-exporter as a scrape job to Prometheus:

Create a file named prometheus-additional-scrape-config.yaml:
vim prometheus-additional-scrape-config.yaml

server:
  extraScrapeConfigs:
    - job_name: 'postgres-exporter'
      static_configs:
        - targets: ['postgres-exporter.monitoring.svc.cluster.local:9187']


[Replace the target with your actual exporter service DNS name (which is correct if it's deployed in the same cluster).]

Then apply it using:
helm upgrade prometheus prometheus-community/prometheus \
  -n monitoring \
  -f prometheus-additional-scrape-config.yaml

NOTE:
it will not affect your old jobs, as long as you only add the new config (i.e., extraScrapeConfigs) in your override file and donâ€™t overwrite other values. 





======================================
ðŸ” Good Practice
If you're worried about unintentional changes, you can:
Backup current values: helm get values prometheus -n monitoring > backup-values.yaml

Merge your changes into a full file: cp backup-values.yaml full-values.yaml

Append your extraScrapeConfigs to full-values.yaml, then:
helm upgrade prometheus prometheus-community/prometheus \
  -n monitoring \
  -f full-values.yaml

To check metrics directly:
kubectl port-forward --address 0.0.0.0 svc/postgres-exporter -n monitoring 9187:9187
http://34.66.117.149:9187/metrics










Search and modify/add these lines:
shared_preload_libraries = 'pg_stat_statements'
track_activity_query_size = 2048

Restart the container for changes to take effect:
docker restart postgres

ðŸ”„ Restart PostgreSQL container if you updated config.
================================================================================
To verify itâ€™s working after restart:
docker exec -it postgres psql -U admin -d monitoringdb

Then: SELECT * FROM pg_stat_statements LIMIT 5;
If it shows data, you're all set!

================================================================================

================================================================================

================================================================================

================================================================================

================================================================================

================================================================================

================================================================================

================================================================================

================================================================================

================================================================================

================================================================================

================================================================================

minikube delete
sudo minikube start --driver=none
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update
helm install prometheus prometheus-community/prometheus -n monitoring --create-namespace
kubectl get svc prometheus-server -n monitoring
http://34.66.117.149:<NodePort>
---
sudo apt update && sudo apt install -y conntrack

sudo minikube start --driver=none
